---
layout: distill
title: AI810 Blog Post (20245589) - Geometric Generative Models
description: An in-depth exploration of geometric generative models, including diffusion, flow matching, generator matching, and their recent extensions to non-Euclidean, discrete, and symmetry-aware domains. This post examines the theoretical underpinnings and practical advancements that enable scalable, structure-preserving, and geometry-aware generation in modern AI systems.

date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
authors:
  - name: Suhyeon Lee
    url: "https://hyn2028.github.io/"
    affiliations: 
      name: Kim Jaechul Graduate School of AI, KAIST

# must be the exact same name as your blogpost
bibliography: 2025-04-28-20245589.bib  


# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Introduction
  - name: Modern Generative Models
    subsections:
    - name: Diffusion Models
    - name: Flow Matching Models
  - name: Non-Euclidean Flow Matching
  - name: Discrete Flow Matching
  - name: Generator Matching
  - name: Equivariant Flow Matching
  - name: Summary

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
  .mycodeblock {
    color:#429472;
  }
  img.animated-gif{
    height: 250px;
    width:auto;
  }
  a {
    color: #87CEFA;
  }

  /* Optional: Change color on hover */
  a:hover {
      color: #1E90FF;
  }
  .callout-block {
      width: 85%;
      margin: 25px auto;           
      padding: 15px 30px;          
      background-color: #1f2937;
      color: #f3f4f6;
      border-radius: 8px;    
  }
---

# Introduction

Generative models like **diffusion** and **flow matching** have become central to modern AI, enabling high-quality data synthesis across images, text, and scientific domains. Diffusion models generate data by reversing noise through stochastic processes, while flow matching models learn deterministic mappings via vector fields. 

Recent research extends these ideas into more generalized and expressive frameworks:

- **Riemannian Flow Matching (RFM)** for curved (non-Euclidean) spaces,
- **Discrete Flow Matching (DFM)** for symbolic data like text and graphs,
- **Generator Matching**, which unifies diffusion, flow, and discrete models through the lens of continuous-time Markov generators,
- **Equivariant Flow Matching (EFM)**, which ensures generative processes respect underlying symmetries such as rotation or permutation invariance.

By combining geometry, structure, and symmetry, these techniques offer new tools for scalable, physically consistent, and domain-aware generation in complex environments.



---

# Modern Generative Models

In recent years, **diffusion models** and **flow models** have emerged as leading approaches in generative modeling due to their remarkable performance across various domains. This section provides a brief overview of these two influential families of generative models.

<div class="l-page">
  <center>
    <img src="{{ 'assets/img/2025-04-28-20245589/0_generative-overview.png' | relative_url }}" 
         alt="Generative Models Overview" 
         style="max-width: 65%; height: auto; object-fit: contain;" />
    <br>
    <i>Source: <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models">lilianweng</a></i>
  </center>
</div>


## Diffusion Models

At a high level, diffusion models learn to reverse a process that gradually corrupts data by adding noise. They model the inverse of a **forward diffusion process**, which transforms real data into pure Gaussian noise over time. During training, the model learns this **reverse denoising process**, enabling it to generate realistic samples from noise during inference.

Well-known diffusion models include **Stable Diffusion v1.5**, **v2**, **SDXL**, **DALLE-2**, and **Imagen**—all widely used for high-quality text-to-image generation.

There are two main types of diffusion models, primarily differing in how they formulate and simulate the diffusion process:

1. **Discrete-time Diffusion Models (DDPMs)**<d-cite key="ho2020denoising"></d-cite>: These models define a Markovian noise process in discrete steps. They are typically trained using variational inference, as popularized by the **Denoising Diffusion Probabilistic Models** (DDPM) framework.

2. **Continuous-time Diffusion Models (Score-based SDEs)**<d-cite key="song2020score"></d-cite>: These models describe the diffusion process using stochastic differential equations (SDEs), treating diffusion as a continuous transformation governed by differential equations. Sampling involves numerically solving a reverse-time SDE.

Although these approaches differ in formulation—discrete and variational vs. continuous and differential—they aim to model the same data distribution and can be seen as complementary perspectives on generative modeling.

In this section, we focus on **score-based SDE models**, examining their mechanisms and effectiveness in generative tasks.

### Core Concept: Score-Based SDEs

Score-based generative models define a continuous-time diffusion process using **stochastic differential equations (SDEs)**<d-cite key="song2020score"></d-cite>. The key idea is to learn the **score function**—the gradient of the log-probability density—of the data distribution as it evolves over time under the forward diffusion process.

<div class="l-page">
  <center>
    <img src="{{ 'assets/img/2025-04-28-20245589/0_score-sde.webp' | relative_url }}" 
         alt="Score-Based SDE" 
         style="max-width: 65%; height: auto; object-fit: contain;" />
    <br>
    <i>Source: <a href="https://arxiv.org/abs/2011.13456">score-SDE</a></i>
  </center>
</div>

### Forward SDE

The forward diffusion process is governed by an SDE of the form:

$$
d\mathbf{x}_t = \mathbf{f}(t, \mathbf{x}_t)dt + g(t)d\mathbf{w}_t,
$$

where:
- $\mathbf{x}_t$ is the state at time $t \in [0, T]$,
- $\mathbf{f}(t, \mathbf{x}_t)$ is the drift coefficient,
- $g(t)$ is the diffusion coefficient,
- $\mathbf{w}_t$ is a standard Wiener process (Brownian motion).

This process gradually corrupts the data $\mathbf{x}_0$ into a noise-like state $\mathbf{x}_T$. Common parameterizations include:
- **VE-SDE**: $\mathbf{f}(t, \mathbf{x}_t) = 0$, $g(t) = \sqrt{\beta(t)}$
- **VP-SDE**: $\mathbf{f}(t, \mathbf{x}_t) = -\frac{1}{2} \beta(t) \mathbf{x}_t$, $g(t) = \sqrt{\beta(t)}$

These formulations ensure that the prior distribution $p_T(\mathbf{x}_T)$ becomes a Gaussian distribution, which contains no information about the original data distribution $p_0(\mathbf{x}_0)$.

### Reverse-Time SDE

To generate data samples, we solve the **reverse-time SDE**<d-cite key="anderson1982reverse"></d-cite>:

$$
d\mathbf{x}_t = \left[\mathbf{f}(t, \mathbf{x}_t) - g(t)^2 \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t)\right] dt + g(t) d\bar{\mathbf{w}}_t,
$$

where $d\bar{\mathbf{w}}_t$ denotes reverse-time Brownian motion and $p_t(\mathbf{x}_t)$ is the marginal density at time $t$.

Sampling proceeds by starting from $\mathbf{x}_T \sim p_T(\mathbf{x}_T)$ (typically Gaussian) and solving the reverse SDE numerically—e.g., via the Euler-Maruyama method—from $t = T$ to $t = 0$. The result is a sample $\mathbf{x}_0 \sim p_0(\mathbf{x}_0)$ drawn from the learned data distribution.

### Denoising Score Matching

The core challenge in solving the reverse-time SDE is estimating the score function $\nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t)$—the gradient of the log-probability at time $t$.

This is done by training a neural network score estimator $s_\theta(\mathbf{x}_t, t)$ to approximate this score, using **denoising score matching**<d-cite key="vincent2011connection"></d-cite>. The loss function is:

$$
\mathcal{L}_{DSM}(\theta) = \mathbb{E}_{t} \left[ \lambda(t) \, \mathbb{E}_{\mathbf{x}_0, \mathbf{x}_t \mid \mathbf{x}_0} \left\| s_\theta(\mathbf{x}_t, t) - \nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t \mid \mathbf{x}_0)\right\|_2^2 \right]
$$

Here:
- $t$ is sampled uniformly from $[0, T]$,
- $\mathbf{x}_0$ is drawn from the data distribution,
- $\mathbf{x}_t$ is a noisy version of $\mathbf{x}_0$, obtained via the forward SDE,
- The conditional distribution $p(\mathbf{x}_{t} \mid \mathbf{x}_0)$ often admits a closed-form expression under common SDE formulations.

This objective guides the network to match the score function at each time step, enabling accurate sampling from the reverse process.



## Flow Matching Models

In parallel with the development of diffusion models, a new class of generative models known as **flow matching models** has emerged. These models aim to directly learn the velocity vector field of an ordinary differential equation (ODE) that maps between two arbitrary probability distributions. Unlike diffusion models, flow matching does not require iterative denoising or sampling from stochastic differential equations. Instead, it leverages principles from optimal transport and continuous normalizing flows, enabling more efficient generation while maintaining high sample quality.

Flow matching has gained significant attention due to its use in notable applications such as **Stable Diffusion 3** and **FLUX**, which apply these techniques to improve scalability, controllability, and performance. As a result, flow matching is increasingly seen as a promising alternative to diffusion models in the development of next-generation foundation models.


### Core Concept: Flow Matching

In flow matching, a **flow** $\psi_t(\mathbf{x}_0)$ is defined as the trajectory of a point $\mathbf{x}_t$ in space as it evolves over time $t \in [0, 1]$, starting from an initial point $\mathbf{x}_0$. This process transports samples from a simple prior distribution (e.g., standard normal) to a target distribution (e.g., real data). The flow is governed by an ODE characterized by a **velocity field** $\mathbf{u}_t(\mathbf{x}_t)$, which describes how points move through space over time:

$$
\frac{d}{dt}\psi_t(\mathbf{x}_0) = \mathbf{u}_t(\psi_t(\mathbf{x}_0))
$$

To generate new samples from the target distribution, one solves the ODE from $t = 0$ to $t = 1$ using the learned velocity field. This can be done using standard numerical ODE solvers like Euler or Runge-Kutta integration.

Unlike diffusion models, which rely on **stochastic SDEs** and denoising score estimation, flow matching models use **deterministic ODEs** with training objectives that directly match the flow of probability mass between distributions.

### Conditional Flow Matching

<div class="l-page">
  <center>
    <img src="{{ 'assets/img/2025-04-28-20245589/0_conditional_flow_matching.png' | relative_url }}" 
         alt="Path design in Flow Matching" 
         style="max-width: 65%; height: auto; object-fit: contain;" />
    <br>
    <i>Source: <a href="https://arxiv.org/abs/2412.06264">Flow Matching Guide and Code</a></i>
  </center>
</div>

The flow defines a **probability path** $$p_t(\mathbf{x}_t)$$ that evolves continuously from the data distribution $p_1$ to the prior distribution $p_0$. By defining a **conditional probability path** $p_{t \mid 1}(\mathbf{x}_t \mid \mathbf{x}_1)$ that conditions on a sample $\mathbf{x}_1$ from the target distribution, the (marginal) probability path path can be written as:

$$
p_t(\mathbf{x}_t) = \int p_{t \mid 1}(\mathbf{x}_t \mid \mathbf{x}_1) p(\mathbf{x}_1) \, d\mathbf{x}_1
$$

Training a velocity field that exactly matches the marginal path is intractable due to the integral over the data distribution. Instead, one can train using a **conditional vector field** $$\mathbf{u}_t(\mathbf{x}_t \mid \mathbf{x}_1)$$, which is a tractable target for the velocity field estimator $\mathbf{u}_\theta(\mathbf{x}_t, t)$. The **conditional flow matching (CFM)** loss is:

$$
\mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{t, \mathbf{x}_1, \mathbf{x}_t \sim p_{t \mid 1}(\cdot \mid \mathbf{x}_1)} \left\| \mathbf{u}_\theta(\mathbf{x}_t, t) - \mathbf{u}_t(\mathbf{x}_t \mid \mathbf{x}_1) \right\|_2^2
$$

It has been shown that the gradient of this conditional loss is equivalent to that of the marginal flow matching loss<d-cite key="lipman2022flow"></d-cite>, making it a valid and efficient training objective. This is analogous to denoising score matching (DSM) in diffusion models, where the score function $$\nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t \mid \mathbf{x}_0)$$ is used to train the marginal score estimator $s_\theta(\mathbf{x}_t, t)$.


### Rectified Flow

In **Rectified Flow**<d-cite key="liu2022flow"></d-cite>, the conditional probability path $p_{t \mid 1}$ is defined in closed form for a Gaussian prior $p_0 = \mathcal{N}(0, I)$,

$$
p_{t \mid 1}(\mathbf{x}_t \mid \mathbf{x}_1) = \mathcal{N}(\mathbf{x}_t; t\mathbf{x}_1, (1 - t)^2 I)
$$

which is the simple linear interpolation between the data sample $\mathbf{x}_1$ and a noise sample $$\boldsymbol{\epsilon} \sim p_0$$ at time $t$.

This corresponds to a linear interpolation with Gaussian noise, forming a simple and tractable setup. Under this formulation, the conditional velocity field becomes:

$$
\mathbf{u}_t(\mathbf{x}_t \mid \mathbf{x}_1) = \frac{\mathbf{x}_1 - \mathbf{x}_t}{1 - t}
$$

Thus, sampling from $p_{t \mid 1}$ is straightforward: given a data sample $\mathbf{x}_1$, sample a time $t \sim \mathcal{U}[0, 1]$, then generate:

$$
\mathbf{x}_t = t\mathbf{x}_1 + (1 - t) \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, I)
$$

The corresponding training loss, known as the **Rectified Flow loss**, simplifies to:

$$
\mathcal{L}_{\text{RF}}(\theta) = \mathbb{E}_{\mathbf{x}_1, \boldsymbol{\epsilon}, t} \left[ \left\| \mathbf{u}_\theta(\mathbf{x}_t, t) - (\mathbf{x}_1 - \boldsymbol{\epsilon}) \right\|^2 \right]
$$

This loss allows the model to learn how to map noise samples toward the data manifold by following a simple rectified flow field. Because of its simplicity and empirical effectiveness, rectified flow has become a foundational component in modern flow matching-based generative models.

### Conclusion

Modern generative models are dominated by two key approaches: diffusion models, which generate data by reversing a noise process via stochastic differential equations, and flow matching models, which use deterministic ordinary differential equations to transport samples from noise to data. While diffusion models are known for their sample quality, they require slow iterative denoising; flow models like Rectified Flow offer faster generation by learning direct mappings using efficient training objectives. Despite their differences, both methods represent powerful, complementary strategies for modeling complex data distributions and continue to drive progress in generative AI.

---

# Non-Euclidean Flow Matching

Flow matching, like most deep learning methods, is traditionally formulated in Euclidean space $\mathbb{R}^d$. However, many real-world data domains—such as molecular structures, protein folding, robotics, and climate systems—reside on non-Euclidean spaces with intricate geometric properties. To properly model such data, flow matching must be extended to curved spaces, leading to the framework of **Riemannian Flow Matching (RFM)**.

### Geometry: From Euclidean to Riemannian

A **Riemannian manifold** $(\mathcal{M}, g)$ is a smooth space where each point has an associated tangent space and an inner product defined by a **Riemannian metric** $g$. This metric generalizes notions of distance, angle, and volume beyond flat Euclidean geometry. As a result, operations like computing gradients or vector fields must be adapted to account for local curvature.

<div class="l-page">
  <center>
    <img src="{{ 'assets/img/2025-04-28-20245589/0_manifold_flow.png' | relative_url }}" 
         alt="Conditional Flows on the Manifold" 
         style="max-width: 35%; height: auto; object-fit: contain;" />
    <br>
    <i>Source: <a href="https://arxiv.org/abs/2412.06264">Flow Matching Guide and Code</a></i>
  </center>
</div>

To generalize flow matching to manifolds, several core components must be redefined:

- ODE: $d\mathbf{x}_t = \mathbf{u}_t(\mathbf{x}_t) dt$
- Conditional velocity field: $\mathbf{u}_t(\mathbf{x}_t \mid \mathbf{x}_1)$
- Conditional flow matching loss:
  $$
  \mathbb{E}_{t, \mathbf{x}_1, \mathbf{x}_t \sim p_{t \mid 1}(\cdot \mid \mathbf{x}_1)} \left\| \mathbf{u}_\theta(\mathbf{x}_t, t) - \mathbf{u}_t(\mathbf{x}_t \mid \mathbf{x}_1) \right\|_g^2
  $$

Foundational work by <d-cite key="lipman2022flow"></d-cite> has laid the groundwork for extending flow matching to Riemannian settings, enabling principled modeling on curved domains.

### Generalizing ODEs on Manifolds

In the Riemannian setting, the flow is governed by an ODE constrained to the manifold:

$$
d\mathbf{x}_t = \mathbf{u}_t(\mathbf{x}_t) dt, \quad \mathbf{u}_t(\mathbf{x}_t) \in T_{\mathbf{x}_t} \mathcal{M},
$$

where $T_{\mathbf{x}_t} \mathcal{M}$ denotes the tangent space at $\mathbf{x}_t$. The velocity field must remain tangent to the manifold at all times.

<div class="l-page">
  <center>
    <img src="{{ 'assets/img/2025-04-28-20245589/0_exponential_map.png' | relative_url }}" 
         alt="Generalized ODEs on Manifolds" 
         style="max-width: 50%; height: auto; object-fit: contain;" />
    <br>
    <i>Source: KAIST AI810 Lecture Notes</i>
  </center>
</div>

Solving this ODE requires the **exponential map** $\exp_{\mathbf{x}_t}$, which maps a tangent vector at $\mathbf{x}_t$ to a point on the manifold:

$$
\mathbf{x}_{t+\Delta t} = \exp_{\mathbf{x}_t} \left( \Delta t \cdot \mathbf{u}_t(\mathbf{x}_t) \right)
$$

This ensures the updated point remains on the manifold, analogous to Euler integration in Euclidean space.

### Generalizing Conditional Velocity Fields

In Euclidean rectified flow, the conditional velocity field connecting $\mathbf{x}_t$ and $\mathbf{x}_1$ is:

$$
\mathbf{u}_t(\mathbf{x}_t \mid \mathbf{x}_1) = \frac{\mathbf{x}_1 - \mathbf{x}_t}{1 - t}
$$

On a Riemannian manifold, this generalizes to:

$$
\mathbf{u}_t(\mathbf{x}_t \mid \mathbf{x}_1) = \frac{\log_{\mathbf{x}_t}(\mathbf{x}_1)}{1 - t}
$$

Here, $\log_{\mathbf{x}_t}(\mathbf{x}_1)$ is the **Riemannian logarithmic map**, which produces the tangent vector at $\mathbf{x}_t$ pointing along the geodesic toward $\mathbf{x}_1$.

The conditional flow itself becomes the geodesic interpolation:

$$
\psi_t(\mathbf{x}_0 \mid \mathbf{x}_1) = \exp_{\mathbf{x}_0} \left( t \cdot \log_{\mathbf{x}_0}(\mathbf{x}_1) \right)
$$

This mirrors linear interpolation in Euclidean space:

$$
\psi_t(\mathbf{x}_0 \mid \mathbf{x}_1) = t \cdot \mathbf{x}_1 + (1 - t) \cdot \boldsymbol{\epsilon}
$$

but respects the curvature of the underlying geometry.

### Riemannian Flow Matching Loss

Given the conditional velocity field, the **Riemannian conditional flow matching loss** becomes:

$$
\mathcal{L}_{\text{RCFM}}(\theta) = \mathbb{E}_{t, \mathbf{x}_1, \mathbf{x}_t \sim p_{t \mid 1}(\cdot \mid \mathbf{x}_1)} \left\| \mathbf{u}_\theta(\mathbf{x}_t, t) - \mathbf{u}_t(\mathbf{x}_t \mid \mathbf{x}_1) \right\|_g^2
$$

The norm $\|\cdot\|_g$ is induced by the Riemannian metric and properly measures the error in the tangent space.

Importantly, just as in Euclidean flow matching, it has been shown that the gradient of this **conditional** loss matches the gradient of the **marginal** loss <d-cite key="lipman2022flow"></d-cite>, making it a valid and effective training objective.

### Conclusion

**Riemannian Flow Matching** (RFM) is a principled extension of flow-based generative modeling to non-Euclidean spaces. By respecting the intrinsic geometry of the data manifold, RFM enables accurate and efficient learning for complex, structured domains. This opens the door to generative models that can faithfully represent the true geometry of scientific, biological, and physical systems—paving the way for next-generation models in manifold-aware machine learning.

---

# Discrete Flow Matching

Discrete data is increasingly important in domains such as natural language processing (NLP), code generation, and graph-based learning. However, conventional flow matching methods are designed for continuous spaces and struggle to handle discrete distributions effectively. To address this limitation, **Discrete Flow Matching (DFM)** adapts the flow matching paradigm to discrete data domains by modeling stochastic transitions using **continuous-time Markov chains (CTMCs)**<d-cite key="campbell2024generative"></d-cite><d-cite key="gat2024discrete"></d-cite>.

This framework enables the generation of sequences, graphs, and other symbolic structures by modeling the evolution of discrete states over continuous time.

We aim to define three core components under the CTMC framework:
1. **Conditional velocity field**: $$u_t(y, x_t \mid x_1)$$
2. **Marginal velocity field** via marginalization trick: $$u_t(y, x_t)$$
3. **Discrete flow matching loss**: A training objective suitable for discrete data


### Continuous-Time Markov Chain (CTMC)

<div class="l-page">
  <center>
    <img src="{{ 'assets/img/2025-04-28-20245589/0_ctmc.png' | relative_url }}" 
         alt="The CTMC model is defined by prescribing rates (velocities) of probability between states." 
         style="max-width: 25%; height: auto; object-fit: contain;" />
    <br>
    <i>Source: <a href="https://arxiv.org/abs/2412.06264">Flow Matching Guide and Code</a></i>
  </center>
</div>

Unlike ODEs used in continuous flow models, **CTMCs** model stochastic transitions in a finite (or countable) state space over continuous time. A CTMC describes a process $$(X_t)_{t \in [0, 1]}$$ with transition dynamics:

$$
p_{t+h \mid t}(y \mid x) = \mathbb{P}(X_{t+h} = y \mid X_t = x) = \delta_{y,x} + h \cdot u_t(y, x) + o(h)
$$

where:
- $$u_t(y, x)$$ is the **transition velocity** from state $$x$$ to $$y$$,
- $$\delta_{y,x}$$ is the Kronecker delta: $$1$$ if $$y = x$$, else $$0$$,
- $$o(h)$$ is a higher-order term satisfying $$o(h)/h \to 0$$ as $$h \to 0$$,
- The velocity matrix must satisfy:
  - $$\sum_y u_t(y, x) = 0$$ (probability conservation),
  - $$u_t(y, x) \geq 0$$ for all $$y \ne x$$.

This formulation mirrors the continuous setting’s velocity field but in the form of transition intensities between discrete states.


### Marginalization Trick

To define the **marginal velocity field** $$u_t(y, x_t)$$, we average over all possible end states $$x_1$$ under the backward conditional $$p_{1 \mid t}(x_1 \mid x_t)$$:

$$
u_t(y, x_t) = \sum_{x_1} u_t(y, x_t \mid x_1) \, p_{1|t}(x_1 \mid x_t) = \mathbb{E}_{x_1 \sim p_{1|t}}[u_t(y, x_t \mid x_1)]
$$

This mirrors the marginalization in continuous flow matching and allows tractable training without evaluating the full joint distribution.



### Discrete Flow Matching Loss

The **Discrete Flow Matching (DFM)** loss is the squared difference between the model’s predicted transition rates and the conditional target velocities:

$$
\mathcal{L}_{\text{DFM}}(\theta) = \mathbb{E}_{t, x_1, x_t \sim p_{t|1}(\cdot \mid x_1)} \left\| u_t^\theta(\cdot, x_t) - u_t(\cdot, x_t \mid x_1) \right\|^2
$$

This loss closely parallels the continuous flow matching loss. Importantly, it can be shown that this conditional loss has the same gradient as the marginal loss, ensuring valid training objectives even in the discrete case.



### Factorized Paths and Velocities

<div class="l-page">
  <center>
    <img src="{{ 'assets/img/2025-04-28-20245589/0_factorized.png' | relative_url }}" 
         alt="Factorized CTMC model allows non-zero rates (velocities) only between states that differ in at-most one coordinate (token)." 
         style="max-width: 25%; height: auto; object-fit: contain;" />
    <br>
    <i>Source: <a href="https://arxiv.org/abs/2412.06264">Flow Matching Guide and Code</a></i>
  </center>
</div>

Directly modeling all possible state transitions is infeasible for high-dimensional discrete spaces (e.g., token sequences) due to practical limitations. To mitigate this, DFM introduces **factorized transitions**<d-cite key="campbell2024generative">, allowing transitions that differ in only one coordinate (e.g., token) at a time:

$$
u_t(y, x_t) = \sum_{i=1}^d \delta(y^{\bar{i}}, x_t^{\bar{i}}) \cdot u_t^i(y^i, x_t)
$$

where:
- $$y^{\bar{i}}$$ denotes all components of $$y$$ except the $$i$$-th,
- $$u_t^i(y^i, x)$$ defines the rate of changing only the $$i$$-th component of $$x$$ to $$y^i$$.

The transition probability over a short time step $$h$$ becomes:

$$
\mathbb{P}(X_{t+h} = y \mid X_t = x) = \prod_{i=1}^d \left[ \delta(y^i, x^i) + h \cdot u_t^i(y^i, x) + o(h) \right]
$$

This factorization significantly reduces computational overhead.



### Conditional Probability Path

To define a simple conditional distribution between a start state $$x_0$$ and a target $$x_1$$, we use a **factorized interpolation path**:

$$
p_t(x_t \mid x_0, x_1) = \prod_{i=1}^d p_t^i(x^i_t \mid x_0, x_1)
$$

with:

$$
p_t^i(x^i_t \mid x_0, x_1) = \kappa_t \cdot \delta(x^i_t, x_1^i) + (1 - \kappa_t) \cdot \delta(x^i_t, x_0^i)
$$

This defines a linear interpolation path in discrete space, with mixing coefficient $$\kappa_t \in [0,1]$$.



### Conditional and Marginal Velocities

From the conditional path, we can define the conditional velocity field:

$$
u_t^i(y^i, x^i_t \mid x_0, x_1) = \frac{\dot{\kappa}_t}{1 - \kappa_t} \left[ \delta(y^i, x_1^i) - \delta(y^i, x^i_t) \right]
$$

Then the marginal velocity field is obtained by integrating over $$x_1$$:

$$
u_t^i(y^i, x_t) = \sum_{x_1} \frac{\dot{\kappa}_t}{1 - \kappa_t} \left[ \delta(y^i, x_1^i) - \delta(y^i, x_t^i) \right] p(x_1 \mid x_t)
$$



### Final Loss Function

Finally, the **coordinate-wise conditional flow matching loss** becomes:

$$
\mathcal{L}_{\text{DFM}}(\theta) = \mathbb{E}_{t, x_1, x_t \sim p_{t|1}(\cdot \mid x_1)} \sum_{i=1}^d \left\| u^{\theta,i}_t (\cdot, x_t) - u_t^i(\cdot, x_t \mid x_1) \right\|^2
$$

This formulation allows for scalable and interpretable training of generative models over structured discrete spaces.



### Conclusion

**Discrete Flow Matching (DFM)** extends flow-based generative modeling to discrete domains by replacing continuous ODEs with continuous-time Markov chains. This allows the modeling of symbolic structures such as text, code, and graphs within a principled generative framework. A key innovation in DFM is the use of **factorized transitions**, which constrain changes to occur in only one coordinate (e.g., one token or node) at a time. This factorization dramatically reduces computational complexity, making it feasible to scale DFM to high-dimensional discrete spaces. By combining conditional velocity fields, marginalization techniques, and coordinate-wise losses, DFM enables efficient and expressive generative modeling across a wide range of discrete data structures.

---

# Generator Matching

So far, we have explored various flow-based generative models—including diffusion, flow matching, and discrete flow models—each operating under specific assumptions about the data domain and process dynamics. Despite their differences, all these models define time-evolving probability distributions governed by **continuous-time Markov processes (CTMPs)**. The unifying abstraction that brings them together is the **infinitesimal generator** $\mathcal{L}_t$ associated with a CTMP. This leads to the powerful framework of **Generator Matching**<d-cite key="holderrieth2024generator"></d-cite>.

The key idea is that, under mild conditions, a CTMP is uniquely characterized by its generator. Therefore, instead of matching trajectories, score functions, or velocity fields directly, we can formulate generative modeling as the task of matching generators—resulting in a principled, general framework.

### Continuous-Time Markov Process (CTMP)

A **continuous-time Markov process** is a stochastic process $(X_t)_{t \in [0, 1]}$ that satisfies the Markov property:

$$
\mathbb{P}(X_{t+h} \in A \mid X_t = x, \text{history}) = \mathbb{P}(X_{t+h} \in A \mid X_t = x)
$$

This implies that the future depends only on the present state. The process is characterized by a **transition kernel**:

$$
p_{t+h \mid t}(A \mid x) = \mathbb{P}(X_{t+h} \in A \mid X_t = x)
$$

Given the initial distribution $p_0$ and the transition kernel $p_{t+h \mid t}$, the process is fully specified.

### Generator: Core Abstraction

The **infinitesimal generator** $\mathcal{L}_t$ of a CTMP is a linear operator that captures the instantaneous dynamics of the process. For a test function $f$, the generator is defined as:

$$
[\mathcal{L}_t f](x) = \lim_{h \to 0} \frac{1}{h} \left( \langle p_{t+h|t}, f \rangle - f(x) \right)
$$

Equivalently, this approximates:

$$
\langle p_{t+h|t}, f \rangle = f(x) + h \cdot [\mathcal{L}_t f](x) + o(h)
$$

This formulation is highly general and provides a compact way to express various types of processes.


### Original Formulations as Generators

Let us now revisit the original models—diffusion, flow, and jump processes—under the generator formalism:

#### 1. **Diffusion Models** (Stochastic Differential Equations)

A typical diffusion model is defined by:

$$
dX_t = \sigma_t(X_t) \, dW_t
$$

where $W_t$ is standard Brownian motion. The corresponding generator is:

$$
[\mathcal{L}_t f](x) = \frac{1}{2} \sigma_t(x)^2 \nabla^2 f(x)
$$

This captures the infinitesimal diffusion (random motion) applied to the data distribution.

#### 2. **Flow Matching Models** (Ordinary Differential Equations)

In flow matching, the dynamics are deterministic:

$$
dX_t = u_t(X_t) \, dt
$$

The generator becomes:

$$
[\mathcal{L}_t f](x) = \nabla f(x)^\top u_t(x)
$$

This describes the transport of probability mass along velocity fields.

#### 3. **Jump Processes** (Discrete Transitions via CTMCs)

In jump models, the process transitions between discrete states with some transition measure $Q_t(dy, x)$:

$$
[\mathcal{L}_t f](x) = \int_y (f(y) - f(x)) \, Q_t(dy, x) dy
$$

Here, $Q_t(dy, x)$ represents the instantaneous rate of jumping from $x$ to $y$. This formulation underpins Discrete Flow Matching.


### Unified Generator Form

All of these models—diffusion, flow, and jump—can be described within a **single unified generator** on Euclidean space:

$$
[\mathcal{L}_t f](x) = \nabla f(x)^\top u_t(x) + \frac{1}{2} \sigma_t(x)^2 \nabla^2 f(x) + \int (f(y) - f(x)) \, Q_t(dy, x)
$$

- The **first term** corresponds to drift (flow),
- The **second term** to stochastic noise (diffusion),
- The **third term** to discrete jumps.

This unified view reveals that all continuous and discrete generative models are specific instantiations of CTMPs with appropriately chosen generators.


### Parameterization of Generators

Generators are functional operators, which are difficult to parameterize directly. Instead, we represent them through their effect on test functions:

$$
[\mathcal{L}_t f](x) = \langle \mathcal{K} f(x), F_t(x) \rangle
$$

Here, $\mathcal{K}$ is a known operator (e.g., gradient, Laplacian), and $F_t(x)$ is a trainable neural network. For example:

- In diffusion: $\mathcal{K} f = \nabla^2 f$, $F_t(x) = \frac{1}{2} \sigma_t(x)^2$
- In flow: $\mathcal{K} f = \nabla f$, $F_t(x) = u_t(x)$

This allows us to unify diverse models under a shared learning objective.


### Marginalization Trick

As with flow matching and discrete flow, we define a **conditional generator** $$\mathcal{L}_{t \mid 1}(x_t \mid x_1)$$, which induces a **conditional path** $$p_{t \mid 1}(x_t \mid x_1)$$. The marginal generator is then obtained via:

$$
\mathcal{L}_t f(x_t) = \mathbb{E}_{x_1 \sim p_{1 \mid t}} [\mathcal{L}_{t \mid 1}f(x_t \mid x_1)]
$$

This mirrors the marginalization in score-based or flow-based models and provides a tractable learning signal.


### Generator Matching Loss

The **Generator Matching Loss** compares the model-predicted generator with a target conditional generator derived from path interpolation:

$$
\mathcal{L}_{\text{CGM}}(\theta) = \mathbb{E}_{t, x_1, x_t \sim p_{t \mid 1}(\cdot \mid x_1)} \left[ D \big( F_t^{x_1}(x_t), F_t^\theta(x_t) \big) \right]
$$

The marginalization condition also holds:

$$
F_t^\theta(x_t) = \mathbb{E}_{x_1 \sim p_{1 \mid t}} \left[ F_t^{x_1}(x_t) \right]
$$

which ensures consistency between conditional and marginal dynamics.


### Conclusion

**Generator Matching** generalizes existing flow-based generative models by focusing on their underlying Markovian dynamics. It abstracts the core modeling challenge into matching **infinitesimal generators**—linear operators that unify flow (deterministic), diffusion (stochastic), and discrete jump (symbolic) processes. This unified view not only streamlines theoretical analysis but also enables novel hybrid architectures that can combine deterministic transport, stochastic diffusion, and symbolic transitions within a single generative framework. As such, generator matching represents a powerful conceptual tool for designing next-generation geometry-aware and structure-aware generative models.

---

# Equivariant Flow Matching

As generative models are increasingly deployed in domains with inherent symmetries—such as 3D molecules (invariant under rotations and translations), particle systems, and physical simulations—**equivariance** has emerged as a critical design principle. An **equivariant generative model** ensures that if input data is transformed by a symmetry group \( G \), the output of the model transforms in a predictable, structured way under a corresponding group action. This promotes data efficiency, physical plausibility, and generalization.

Flow matching models can be extended to incorporate such symmetries through a framework called **Equivariant Flow Matching (EFM)**<d-cite key="klein2023equivariant"></d-cite>. The core idea is to adapt flow matching losses so that they remain equivariant under group actions.


### Multisample Coupling for Equivariance

In standard **Conditional Flow Matching (CFM)**, we construct couplings between samples from the prior $$ p_0 $$ and the data distribution $$ p_1 $$. The only constraint is that the joint coupling $$ q(x_0, x_1) $$ satisfies the marginal consistency:

$$
\int q(x_0, x_1) \, dx_1 = p_0(x_0), \quad \int q(x_0, x_1) \, dx_0 = p_1(x_1)
$$

However, not all couplings are equally desirable. To minimize discretization error during ODE integration, it is advantageous for the interpolated paths to be **as straight and short as possible**. This can be formalized via the **path energy functional**:

$$
\min_p \int \left\| u_t(x_t) \right\|_2^2 \, dt = \min_p \int \mathbb{E}_p [\left\| u_t(x_t|x_0,x_1) \right\|_2^2 ] dt
$$

Unfortunately, solving this exactly is intractable in high dimensions. Instead, we can upper-bound the path energy using a **quadratic cost** between endpoints:

$$
\min_q \int \left\| u_t(x_t) \right\|_2^2 \, dt \leq \min_q \int \left\| x_1 - x_0 \right\|_2^2
$$

This reduces to a **minimum-cost perfect matching** problem between samples drawn from $$ p_0 $$ and $$ p_1 $$, for which efficient combinatorial solvers exist. Such **multisample matching**<d-cite key="pooladian2023multisample"></d-cite> allows one to find high-quality pairings that improve training stability and convergence.


### Equivariance via Group-Optimal Coupling

To enforce **equivariance**, we modify the matching step to respect the symmetry group \( G \) under which the data is invariant or equivariant. This is achieved by introducing group elements into the coupling optimization.

1. **Sample** a noise vector $$ x_0 \sim p_0 $$ and a data point $$ x_1 \sim p_1 $$.
2. **Optimize over group elements** to find the best-aligned transformation:
   $$
   g^* = \arg\min_{g \in G} \left\| x_0 - \rho(g) x_1 \right\|_2^2
   $$
   where $$ \rho(g) $$ is the representation of group element $$ g $$ acting on $$ x_1 $$.
3. **Construct a coupling** $$ (x_0, \rho(g^*) x_1) $$ and use this aligned pair in the conditional flow matching loss.

This guarantees that the learned velocity field respects group symmetry:
- When the input is transformed by $$ g $$, the flow changes consistently via $$ \rho(g) $$,
- The learned model becomes **equivariant by construction**.


### Conclusion

**Equivariant Flow Matching (EFM)** introduces group symmetry into the flow matching framework by aligning data through **optimal group element couplings**. By minimizing transport cost over group-transformed pairs, EFM ensures that the model respects data symmetries such as rotation or permutation invariance. This leads to more **data-efficient**, **physically consistent**, and **generalizable** generative models, especially in scientific and geometric domains. As a result, EFM plays a crucial role in extending the power of flow-based generation to structured, symmetry-rich environments.


---


# Summary

In this post, we explored the landscape of **geometric generative models**, focusing on two major paradigms: **diffusion models** and **flow matching models**. We reviewed how diffusion models leverage stochastic processes and score estimation, while flow matching uses deterministic ODEs to learn efficient sample transformations.

We then expanded these frameworks in several important directions:
- **Riemannian Flow Matching (RFM)** adapts flow models to non-Euclidean spaces, enabling generation on curved manifolds relevant to physical and biological systems.  
- **Discrete Flow Matching (DFM)** extends flow-based methods to symbolic data using continuous-time Markov chains, making text and graphs accessible to generative modeling.  
- **Generator Matching** generalizes the entire family of models through the unifying abstraction of infinitesimal generators, encompassing flows, diffusions, and jumps.  
- **Equivariant Flow Matching (EFM)** enforces symmetry constraints, ensuring that learned generative processes respect transformations like rotations or permutations.

Together, these developments illustrate the growing power and flexibility of **geometry-aware, structure-aware, and symmetry-respecting** generative models—enabling AI systems to more accurately model and generate data across diverse and complex domains.
